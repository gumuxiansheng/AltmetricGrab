{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab Dimensions Data\n",
    "\n",
    "https://app.dimensions.ai/discover/publication\n",
    "\n",
    "This notebook handles the downloads of Dimensions data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Journals"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_folder = 'Journals_Articles'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_file = f'{origin_folder}/ABS期刊汇总_DimentionsSearch.csv'\n",
    "journals = pd.read_csv(journals_file, usecols=['search title', 'search id'])\n",
    "journals = journals.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(journals)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_index = '1-10'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = journals_index.split('-')[0]\n",
    "end = journals_index.split('-')[1]\n",
    "journals = journals[start - 1:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Request Functions\n",
    "\n",
    "This part contains functions we need to fetch the web data and should also handle the exceptions while fetching here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "from requests import ConnectionError, ReadTimeout\n",
    "\n",
    "def grab_from_url_content(url):\n",
    "    headers = {'Accept': '* / *',\n",
    "               'Accept-Language': 'zh-TW, zh; q=0.9, en-US; q=0.8, en; q=0.7, zh-CN; q=0.6',\n",
    "               'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3610.2 Safari/537.36'\n",
    "               }\n",
    "    rescontent = ''\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        rescontent = res.text\n",
    "    except ConnectionError as ce:\n",
    "        print('ConnectionError: ' + str(ce))\n",
    "        return grab_from_url_content(url)\n",
    "    except ReadTimeout as rte:\n",
    "        print('ReadTimeout: ' + str(rte))\n",
    "        return grab_from_url_content(url)\n",
    "\n",
    "    return rescontent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parser\n",
    "\n",
    "We will get the html content from the url which is not listed as we want it be, so we need parser to parse them into listed data, in json form."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class DimensionsHTMLParser(HTMLParser):\n",
    "    articles = []\n",
    "    in_article = False\n",
    "    is_next_trigger = False\n",
    "    next_trigger_url = ''\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'article':\n",
    "            self.in_article = True\n",
    "        \n",
    "        if self.in_article:\n",
    "            for attr in attrs:\n",
    "                if attr[0] == 'data-doc':\n",
    "                    self.articles.append(json.loads(attr[1]))\n",
    "                    break\n",
    "\n",
    "        if tag == 'a':\n",
    "            for attr in attrs:\n",
    "                if (attr[0] == 'class') and (attr[1] == 'nextPage-trigger'):\n",
    "                    self.is_next_trigger = True\n",
    "                if self.is_next_trigger and (attr[0] == 'href'):\n",
    "                    self.next_trigger_url = f'https://app.dimensions.ai{attr[1]}&{search_params}'\n",
    "                    break\n",
    "        return\n",
    "                    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'article':\n",
    "            self.in_article = False\n",
    "        self.is_next_trigger = False\n",
    "        return\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        pass\n",
    "\n",
    "    def handle_comment(self, data):\n",
    "        pass\n",
    "\n",
    "    def handle_entityref(self, name):\n",
    "        pass\n",
    "\n",
    "    def handle_charref(self, name):\n",
    "        pass\n",
    "\n",
    "    def handle_decl(self, data):\n",
    "        pass\n",
    "\n",
    "parser = DimensionsHTMLParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab Data\n",
    "\n",
    "The data fetching starts from here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for row in journals.iterrows():\n",
    "    journal_search = row['search id']\n",
    "    # This is very important for data fetching, generally you get url in the form as 'https://app.dimensions.ai/discover/publication?and_facet_for=2209&and_facet_for=2202&or_facet_source_title=jour.1082997', and search_params is just the substring after '?'.\n",
    "    # Here is where we need to change for each search.\n",
    "    search_params = f'or_facet_year=2014&or_facet_year=2015&or_facet_year=2016&or_facet_year=2017&or_facet_source_title={journal_search}'\n",
    "    grab_url = f'https://app.dimensions.ai/discover/publication?{search_params}'\n",
    "\n",
    "    parser.feed(grab_from_url_content(grab_url))\n",
    "    next_trigger_anchor = ''\n",
    "    i = 0\n",
    "    while parser.next_trigger_url != next_trigger_anchor:\n",
    "        if i % 100 == 0:\n",
    "            print(str(i))\n",
    "        i+=1\n",
    "        next_trigger_anchor = parser.next_trigger_url\n",
    "        parser.feed(grab_from_url_content(parser.next_trigger_url))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(parser.articles))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{origin_folder}/articles_{journals_index}.json\", \"w\") as dump_f:\n",
    "        dump_f.write(json.dumps(parser.articles))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{origin_folder}/articles_{journals_index}.json') as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "df_articles = pd.DataFrame(columns=['SO', 'SO_id', 'DI', 'dimensions_id'])\n",
    "\n",
    "for article in articles:\n",
    "    if 'doi' not in article:\n",
    "        continue\n",
    "    if article['doi'] in df_articles['DI']:\n",
    "        continue\n",
    "    df_article = pd.DataFrame(columns=['SO', 'SO_id', 'DI', 'dimensions_id'])\n",
    "    df_article['SO'] = [article['source_title']]\n",
    "    df_article['SO_id'] = [article['source_title_id']]\n",
    "    df_article['DI'] = [article['doi']]\n",
    "    df_article['dimensions_id'] = [article['id']]\n",
    "    df_articles = df_articles.append(df_article, ignore_index=True)\n",
    "\n",
    "df_articles.to_csv(f'{origin_folder}/articles_{journals_index}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}