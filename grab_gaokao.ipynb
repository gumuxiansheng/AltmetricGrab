{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from requests import ConnectionError, ReadTimeout\n",
    "\n",
    "class Request(object):\n",
    "    def __init__(self, request_session):\n",
    "        self.request_session = request_session\n",
    "    \n",
    "    def get(self, url, headers={'Accept': '* / *',\n",
    "               'Accept-Language': 'zh-TW, zh; q=0.9, en-US; q=0.8, en; q=0.7, zh-CN; q=0.6',\n",
    "               'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3610.2 Safari/537.36'\n",
    "               }, timeout=10, allow_redirects=False):\n",
    "        try:\n",
    "            print(url)\n",
    "            headers['Host'] = urllib.request.splithost(urllib.request.splittype(url)[1])[0]\n",
    "            res = self.request_session.get(url, headers=headers, timeout=timeout, allow_redirects=allow_redirects)\n",
    "        except ConnectionError as ce:\n",
    "            print('ConnectionError: ' + str(ce))\n",
    "            return self.get(url=url, headers=headers, timeout=timeout, allow_redirects=allow_redirects)\n",
    "        except ReadTimeout as rte:\n",
    "            print('ReadTimeout: ' + str(rte))\n",
    "            return self.get(url=url, headers=headers, timeout=timeout, allow_redirects=allow_redirects)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def post(self, url, data, headers={'Accept': '* / *',\n",
    "               'Accept-Language': 'zh-TW, zh; q=0.9, en-US; q=0.8, en; q=0.7, zh-CN; q=0.6',\n",
    "               'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3610.2 Safari/537.36'\n",
    "               }, timeout=10, allow_redirects=False):\n",
    "        try:\n",
    "            headers['host'] = urllib.request.splithost(urllib.request.splittype(url)[1])[0]\n",
    "            res = self.request_session.post(url, data=data, headers=headers, timeout=timeout, allow_redirects=allow_redirects)\n",
    "        except ConnectionError as ce:\n",
    "            print('ConnectionError: ' + str(ce))\n",
    "            return self.post(url=url, data=data, headers=headers, timeout=timeout, allow_redirects=allow_redirects)\n",
    "        except ReadTimeout as rte:\n",
    "            print('ReadTimeout: ' + str(rte))\n",
    "            return self.post(url=url, data=data, headers=headers, timeout=timeout, allow_redirects=allow_redirects)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def get_cookie_dict(self):\n",
    "        return requests.utils.dict_from_cookiejar(self.request_session.cookies)\n",
    "\n",
    "    def clear_cookie(self):\n",
    "        self.request_session.cookies.clear()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colleges = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_size = 20\n",
    "for page in range(0, 60):\n",
    "    url = f'https://gaokao.chsi.com.cn/zyk/pub/myd/specAppraisalTop.action?xlcc=bk&start={page*page_size}'\n",
    "    req = Request(requests.Session())\n",
    "\n",
    "    res = req.get(url, timeout=20)\n",
    "\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    query_result = soup.find(id = 'queryResult')\n",
    "    query_result = query_result.findChildren('tr')\n",
    "\n",
    "    for item in query_result:\n",
    "        a = item.find('a', title='点击查看院校信息')\n",
    "        if a == None:\n",
    "            continue\n",
    "        df_college = pd.DataFrame()\n",
    "        df_college['Name'] = [a.string.strip()]\n",
    "        df_college['DetailUrl'] = [a['href']]\n",
    "\n",
    "        a = item.find('a', attrs={'class':'check_detail'})\n",
    "        df_college['SpecialtyDetailUrl'] = [a['href']]\n",
    "\n",
    "        df_colleges = df_colleges.append(df_college, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colleges"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colleges.to_csv('SUSTC_Journals/本科院校名录.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = Request(requests.Session())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialties = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(0, len(df_colleges)):\n",
    "    if df_colleges.iloc[i]['Name'] in list(df_specialties['CollegeName']):\n",
    "        continue\n",
    "    url = 'https://gaokao.chsi.com.cn' + df_colleges.iloc[i]['SpecialtyDetailUrl']\n",
    "    res = req.get(url, timeout=40)\n",
    "\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    query_result = soup.find(id = 'queryResult')\n",
    "    query_result = query_result.findChildren('tr', align='left')\n",
    "\n",
    "    # satisfy_cats = ['综合', '办学', '教学', '就业']\n",
    "    satisfy_cats = ['Zonghe', 'Banxue', 'Jiaoxue', 'Jiuye']\n",
    "\n",
    "    for item in query_result:\n",
    "        td = item.find('td', attrs={'class':'first_td'})\n",
    "        if td == None:\n",
    "            continue\n",
    "        df_specialty = pd.DataFrame()\n",
    "        df_specialty['CollegeName'] = [df_colleges.iloc[i]['Name']]\n",
    "        df_specialty['Specialty'] = [td.string.strip()]\n",
    "\n",
    "        query_subs = item.findAll('table', attrs={'class':'zymydMoreTable'})\n",
    "\n",
    "        for index, sub in enumerate(query_subs):\n",
    "            satisfy_cat = satisfy_cats[index]\n",
    "            avg_rank = sub.find('span', attrs={'class':'avg_rank'})\n",
    "            if (avg_rank != None) and (avg_rank.string != None):\n",
    "                df_specialty[f'{satisfy_cat}_avg_rank'] = [avg_rank.string.strip()]\n",
    "\n",
    "            vote_num_detail = sub.find('span', attrs={'class':'vote_num_detail'})\n",
    "            if (vote_num_detail != None) and (vote_num_detail.string != None): \n",
    "                df_specialty[f'{satisfy_cat}_vote_num'] = [vote_num_detail.string.strip()]\n",
    "\n",
    "            sub_ratings = sub.findAll('div', attrs={'class':'progress_bar'})\n",
    "            for sub_index, sub_rating in enumerate(sub_ratings):\n",
    "                df_specialty[f'{satisfy_cat}_{5-sub_index}_star_percent'] = [sub_ratings[sub_index]['style'].split(':')[1].strip()[:-1]]\n",
    "\n",
    "        df_specialties = df_specialties.append(df_specialty, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialties"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialties.to_csv('SUSTC_Journals/本科院校专业满意度.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialties['Specialty'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高考网专业分数线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = Request(requests.Session())\n",
    "col_names = ['Specialty Name', 'College Name', 'Average Grade', 'Max Grade', 'Region', 'Category', 'Year', 'Level']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_grades(year):\n",
    "    df_grade = pd.DataFrame()\n",
    "    total_page = 25000\n",
    "    for page in range(7401, 15000):\n",
    "        if page > total_page:\n",
    "            break\n",
    "        url = f'http://college.gaokao.com/spepoint/y{year}/p{page}'\n",
    "        res = req.get(url, timeout=20)\n",
    "        req.clear_cookie()\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        if total_page > 24000:\n",
    "            query_result = soup.find(id='qx')\n",
    "            if query_result != None:\n",
    "                total_page = int(query_result.find(id='pagenum').previousSibling.string.split('/')[1].split('页')[0])\n",
    "                \n",
    "        query_result = soup.findChildren('tr', attrs={'class':re.compile('sz*')})\n",
    "\n",
    "        for item in query_result:\n",
    "            df_gradex = pd.DataFrame()\n",
    "            subs = item.find_all('td')\n",
    "            for index in range(0, 7):\n",
    "                df_gradex[col_names[index]] = [subs[index].string]\n",
    "            df_grade = df_grade.append(df_gradex, ignore_index=True)\n",
    "\n",
    "        if (page % 200 == 0) or (page == total_page):\n",
    "            df_grade = df_grade.drop_duplicates()\n",
    "            df_grade.to_csv(f'SUSTC_Journals/本科院校专业录取分数线{year}_{page}.csv', index=False)\n",
    "            df_grade = pd.DataFrame()\n",
    "            gc.collect()\n",
    "        \n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "http://college.gaokao.com/spepoint/y2009/p7401\nhttp://college.gaokao.com/spepoint/y2010/p7401http://college.gaokao.com/spepoint/y2011/p7401\n\nhttp://college.gaokao.com/spepoint/y2012/p7401\nhttp://college.gaokao.com/spepoint/y2013/p7401\nhttp://college.gaokao.com/spepoint/y2014/p7401\nhttp://college.gaokao.com/spepoint/y2015/p7401\nhttp://college.gaokao.com/spepoint/y2016/p7401\nhttp://college.gaokao.com/spepoint/y2016/p7402\nhttp://college.gaokao.com/spepoint/y2013/p7402\nhttp://college.gaokao.com/spepoint/y2015/p7402\nhttp://college.gaokao.com/spepoint/y2014/p7402\nhttp://college.gaokao.com/spepoint/y2009/p7402\nhttp://college.gaokao.com/spepoint/y2012/p7402\nhttp://college.gaokao.com/spepoint/y2011/p7402\nhttp://college.gaokao.com/spepoint/y2010/p7402\n"
    }
   ],
   "source": [
    "%%time\n",
    "threads = []\n",
    "for year in range(2009, 2017):\n",
    "    threadx = threading.Thread(target=grab_grades, name=f'Thread{year}', args=(year,))\n",
    "    threadx.start()\n",
    "    threads.append(threadx)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高考网地区批次线"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = Request(requests.Session())\n",
    "col_names = ['Year', 'Region', 'Category', 'Batch Name', 'Min Grade']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_batch_grades():\n",
    "    df_grade = pd.DataFrame()\n",
    "    total_page = 700\n",
    "    for page in range(1, 600):\n",
    "        if page > total_page:\n",
    "            break\n",
    "        url = f'http://college.gaokao.com/areapoint/p{page}'\n",
    "        res = req.get(url, timeout=20)\n",
    "        req.clear_cookie()\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        if total_page > 500:\n",
    "            query_result = soup.find(id='qx')\n",
    "            if query_result != None:\n",
    "                total_page = int(query_result.find(id='pagenum').previousSibling.string.split('/')[1].split('页')[0])\n",
    "                \n",
    "        query_result = soup.findChildren('tr', attrs={'class':re.compile('sz*')})\n",
    "\n",
    "        for item in query_result:\n",
    "            df_gradex = pd.DataFrame()\n",
    "            subs = item.find_all('td')\n",
    "            for index in range(0, 5):\n",
    "                df_gradex[col_names[index]] = [subs[index].string]\n",
    "            df_grade = df_grade.append(df_gradex, ignore_index=True)\n",
    "\n",
    "        if (page % 50 == 0) or (page == total_page):\n",
    "            df_grade = df_grade.drop_duplicates()\n",
    "            df_grade.to_csv(f'SUSTC_Journals/地区批次录取分数线_{page}.csv', index=False)\n",
    "            df_grade = pd.DataFrame()\n",
    "            gc.collect()\n",
    "        \n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grab_batch_grades()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}