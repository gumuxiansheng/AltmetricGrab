{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab Altmetric.com Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Request Functions\n",
    "This part contains functions we need to fetch the web data and should also handle the exceptions while fetching here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grab_util import *\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_folder = 'data/outputs/OR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab altmetric.com Ids"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_altmetric_id(doi):\n",
    "    detail_id = ''\n",
    "    res = grab_from_url_json('https://api.altmetric.com/v1/doi/' + doi)\n",
    "    if res is not None:\n",
    "        detail_id = res['altmetric_id']\n",
    "    return detail_id"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_articles_df = pd.read_excel(f'{origin_folder}/all.xlsx', usecols=['SO', 'DI'])\n",
    "all_articles_df = pd.read_csv(f'{origin_folder}/all.csv', usecols=['SO', 'DI'])\n",
    "df = pd.DataFrame(columns=['DI', 'altmetric_id'])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, doi in enumerate(all_articles_df['DI']):\n",
    "    if doi in df['DI'].values:\n",
    "        continue\n",
    "    if index % 100 == 0:\n",
    "        print(index)\n",
    "    dfx = pd.DataFrame(columns=['DI', 'altmetric_id'])\n",
    "    dfx['DI'] = [doi]\n",
    "    dfx['altmetric_id'] = [get_altmetric_id(str(doi))]\n",
    "    df = df.append(dfx, ignore_index=True)\n",
    "\n",
    "df.to_csv(f'{origin_folder}/all_altmetric_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab altmetric.com Details"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_detail_altmetric(doi, altmetric_id, so):\n",
    "    df = pd.DataFrame()\n",
    "    b_list = ['news outlets', 'blogs', 'policy', 'tweeters', 'weibo', 'facebook pages', 'wikipedia', 'redditors', 'f1000', 'video uploader', 'dimensions_citation', 'mendeley', 'citeulike']\n",
    "\n",
    "    df['DI'] = [doi]\n",
    "    df['altmetric_id'] = [altmetric_id]\n",
    "    if altmetric_id != '' and not math.isnan(altmetric_id):\n",
    "        altmetric_id = int(altmetric_id)\n",
    "        news_anchor = 'news</dt><dd><a href=\"/details/' + str(altmetric_id) + '/news\"><strong>'\n",
    "        blogs_anchor = 'blogs</dt><dd><a href=\"/details/' + str(altmetric_id) + '/blogs\"><strong>'\n",
    "        policy_anchor = 'policy</dt><dd><a href=\"/details/' + str(altmetric_id) + '/policy-documents\"><strong>'\n",
    "        twitter_anchor = 'twitter</dt><dd><a href=\"/details/' + str(altmetric_id) + '/twitter\"><strong>'\n",
    "        weibo_anchor = 'weibo</dt><dd><a href=\"/details/' + str(altmetric_id) + '/weibo\"><strong>'\n",
    "        facebook_anchor = 'facebook</dt><dd><a href=\"/details/' + str(altmetric_id) + '/facebook\"><strong>'\n",
    "        wikipedia_anchor = 'wikipedia</dt><dd><a href=\"/details/' + str(altmetric_id) + '/wikipedia\"><strong>'\n",
    "        redditors_anchor = 'reddit</dt><dd><a href=\"/details/' + str(altmetric_id) + '/reddit\"><strong>'\n",
    "        f1000_anchor = 'f1000</dt><dd><a href=\"/details/' + str(altmetric_id) + '/f1000\"><strong>'\n",
    "        video_anchor = 'video</dt><dd><a href=\"/details/' + str(altmetric_id) + '/video\"><strong>'\n",
    "        dimensions_citation_anchor = 'dimensions_citation</dt><dd><a href=\"/details/' + str(altmetric_id) + '/citations\"><strong>'\n",
    "        mendeley_anchor = 'mendeley</dt><dd><a href=\"/details/' + str(altmetric_id) + '#mendeley-demographics\"><strong>'\n",
    "        citeulike_anchor = 'citeulike</dt><dd><strong>'\n",
    "        c_list = [news_anchor, blogs_anchor, policy_anchor, twitter_anchor, weibo_anchor, facebook_anchor, wikipedia_anchor, redditors_anchor, f1000_anchor, video_anchor, dimensions_citation_anchor, mendeley_anchor, citeulike_anchor]\n",
    "\n",
    "        end_anchor = '</strong>'\n",
    "\n",
    "        res = grab_from_url_content('https://www.altmetric.com/details/' + str(altmetric_id))\n",
    "        if res is not None:\n",
    "\n",
    "            for i in range(0, len(c_list)):\n",
    "                start_index = res.find(c_list[i])\n",
    "                if start_index > 0:\n",
    "                    start_index += len(c_list[i])\n",
    "                    number = 0\n",
    "                    end_index = res.find(end_anchor, start_index, start_index + 100)\n",
    "                    number_temp = res[start_index: end_index]\n",
    "\n",
    "                    if number_temp is not '':\n",
    "                        number = number_temp\n",
    "                    df[b_list[i]] = int(number)\n",
    "                else:\n",
    "                    df[b_list[i]] = 0\n",
    "        else:\n",
    "            for i in range(0, len(b_list)):\n",
    "                df[b_list[i]] = 0\n",
    "    else:\n",
    "        for i in range(0, len(b_list)):\n",
    "            df[b_list[i]] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_articles_df = pd.read_csv(f'{origin_folder}/all_altmetric_id.csv')\n",
    "df = pd.DataFrame(columns=['DI'])\n",
    "for index, row in all_articles_df.iterrows():\n",
    "    if row['DI'] in df['DI'].values:\n",
    "        continue\n",
    "    if index % 100 == 0:\n",
    "        print(index)\n",
    "    dfx = grab_detail_altmetric(row['DI'], row['altmetric_id'])\n",
    "    df = df.append(dfx, ignore_index=True)\n",
    "\n",
    "df.to_csv(f'{origin_folder}/all_altmetric_detail.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab altmetric.com's Tweets Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parser\n",
    "We will get the html content from the url which is not listed as we want it be, so we need parser to parse them into listed data, in json form."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class AltmetricHTMLParser(HTMLParser):\n",
    "    tweets = []\n",
    "    retweets = []\n",
    "    articles = {'tweets': tweets, 'retweets': retweets}\n",
    "    in_article = False\n",
    "    is_reply = False\n",
    "    has_article = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'body':\n",
    "            self.has_article = False\n",
    "        if tag == 'article':\n",
    "            self.has_article = True\n",
    "            self.in_article = True\n",
    "\n",
    "        if self.in_article and (tag == 'a'):\n",
    "            for attr in attrs:\n",
    "                if (attr[0] == 'class') and (attr[1] == 'reply'):\n",
    "                    self.is_reply = True\n",
    "                if self.is_reply and (attr[0] == 'href'):\n",
    "                    self.tweets.append(attr[1].split('=')[1])\n",
    "                    break\n",
    "        return\n",
    "                    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'article':\n",
    "            self.in_article = False\n",
    "        self.is_reply = False\n",
    "        return\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        pass\n",
    "\n",
    "    def handle_comment(self, data):\n",
    "        pass\n",
    "\n",
    "    def handle_entityref(self, name):\n",
    "        pass\n",
    "\n",
    "    def handle_charref(self, name):\n",
    "        pass\n",
    "\n",
    "    def handle_decl(self, data):\n",
    "        pass\n",
    "\n",
    "parser = AltmetricHTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_str = '<html><body>You are being <a href=\"https://www.altmetric.com/details/4236878\">redirected</a>.</body></html>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grab Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(f'{origin_folder}/all_altmetric_id.csv', usecols=['DI', 'altmetric_id'], dtype={'altmetric_id':str})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# articles_dict = {}\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['altmetric_id'] == '' or str(row['altmetric_id']).lower() == 'nan':\n",
    "#         articles_dict[row['DI']] = {'altmetric_id': '', 'twitter_num': 0, 'tweets': []}\n",
    "#         continue\n",
    "#     # print(row['DI'], row['altmetric_id'])\n",
    "#     grab_url = f'https://www.altmetric.com/details/{row[\"altmetric_id\"]}/twitter/page:'\n",
    "#     print(str(index), grab_url)\n",
    "#     parser.articles = []\n",
    "#     for i in range(1, 100000):\n",
    "#         parser.feed(grab_from_url_content(grab_url + str(i), headers=headers))\n",
    "#         print('page: ', str(i))\n",
    "#         if not parser.has_article:\n",
    "#             break\n",
    "#     articles_dict[row['DI']] = {'altmetric_id': row['altmetric_id'], 'twitter_num': len(parser.articles), 'tweets': parser.articles}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_dict = {}\n",
    "def get_mention(source):\n",
    "    headers = {\n",
    "        'cookie': \"_ga=GA1.2.501259425.1572141776; weibo_license_acknowledgement=false; explorer_user=WWs2eDNOOUVjVHJTcHRjSm9Oc0ZuZEh0eDRkTmlPd0tCWFMzODNyNlIweFBvdjllM3RkcmF0RVdtRWdvRDN4Ny0tQlNpeTBsWU5NK0ZNTnFpK1FpVFUzdz09--8bac413ca9fd623b9918b311729130cfbb4e66e1; _altmetric-explorer_session=eExKU3g1UmJhWXpXbmVtS05YZmxVS3YyaVNDSGdPL2JyaWVYaE9mRTdPT3RlYlRLaTdrRmRWcnZpeFMrMFJLb0FIVDBvNHI3N09MWS9HZDhDRDdIbTZPZElvMWNPNzMvTm5JVkovWThiQVplZVdTRlZheDBWYWdvd2lIMXo2NHJUZkFJd2MxcWpZSmpCaXVXRW9CYlZnZ1l4L1lXMGhIa1k1d0o1bllvS09uWDBQb3JXMWUrazZ2YWVwaUwzei93cDdUZkhwanFtSy9lSkxLUmdRdEIrZz09LS1DckIxZFJZUVBWNmhsTEJXcWtkR2x3PT0%3D--884ad24217914b545556a68678c2d803c65ce9cf; intercom-session-9dnltu6y=alcwSlZxZHMzZlY3Wm8yaFpvWFg1OGxEYVZ4MXRRUDMrbkk1Q01Bdm5XNXJkQXNZcVAyeEZYVHFYS3RUbWJSbi0tclFPUDlZcUVpMFZKQVlxdlMwRllVQT09--46bb4ebcf64063d6a3712dd82a1d2bff57eed87f, _ga=GA1.2.501259425.1572141776; weibo_license_acknowledgement=false; explorer_user=WWs2eDNOOUVjVHJTcHRjSm9Oc0ZuZEh0eDRkTmlPd0tCWFMzODNyNlIweFBvdjllM3RkcmF0RVdtRWdvRDN4Ny0tQlNpeTBsWU5NK0ZNTnFpK1FpVFUzdz09--8bac413ca9fd623b9918b311729130cfbb4e66e1; _altmetric-explorer_session=eExKU3g1UmJhWXpXbmVtS05YZmxVS3YyaVNDSGdPL2JyaWVYaE9mRTdPT3RlYlRLaTdrRmRWcnZpeFMrMFJLb0FIVDBvNHI3N09MWS9HZDhDRDdIbTZPZElvMWNPNzMvTm5JVkovWThiQVplZVdTRlZheDBWYWdvd2lIMXo2NHJUZkFJd2MxcWpZSmpCaXVXRW9CYlZnZ1l4L1lXMGhIa1k1d0o1bllvS09uWDBQb3JXMWUrazZ2YWVwaUwzei93cDdUZkhwanFtSy9lSkxLUmdRdEIrZz09LS1DckIxZFJZUVBWNmhsTEJXcWtkR2x3PT0%3D--884ad24217914b545556a68678c2d803c65ce9cf; intercom-session-9dnltu6y=alcwSlZxZHMzZlY3Wm8yaFpvWFg1OGxEYVZ4MXRRUDMrbkk1Q01Bdm5XNXJkQXNZcVAyeEZYVHFYS3RUbWJSbi0tclFPUDlZcUVpMFZKQVlxdlMwRllVQT09--46bb4ebcf64063d6a3712dd82a1d2bff57eed87f; Cookie_1=value\",\n",
    "        'User-Agent': \"PostmanRuntime/7.19.0\",\n",
    "        'Accept': \"*/*\",\n",
    "        'Cache-Control': \"no-cache\",\n",
    "        'Postman-Token': \"30d13c93-aa9a-44e1-84e0-8876f98ae1ba,b45c8eb6-7f77-4c48-b961-921c89280079\",\n",
    "        'Host': \"www.altmetric.com\",\n",
    "        'Accept-Encoding': \"gzip, deflate\",\n",
    "        'Connection': \"keep-alive\",\n",
    "        'cache-control': \"no-cache\"\n",
    "    }\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['DI'] in articles_dict:\n",
    "            continue\n",
    "        if index % 100 == 0:\n",
    "            print(str(index))\n",
    "        if row['DI'] == '' or str(row['DI']).lower() == 'nan' or row['altmetric_id'] == '' or str(row['altmetric_id']).lower() == 'nan':\n",
    "            articles_dict[row['DI']] = {'altmetric_id': '', f'{source}_num': 0, f'{source}s': []}\n",
    "            continue\n",
    "\n",
    "        articles = []\n",
    "        grab_url = f'https://www.altmetric.com/explorer/json_data/mentions?identifier={row[\"DI\"]}&mention_sources%5B%5D=type%3A{source}&scope=all&page='\n",
    "        for i in range(1, 100000):\n",
    "            articles_json = grab_from_url_json(grab_url + str(i), headers=headers)\n",
    "            articles.extend(articles_json['data'])\n",
    "            # print('page: ', str(i))\n",
    "            if articles_json['lastPage']:\n",
    "                break\n",
    "            \n",
    "        articles_dict[row['DI']] = {'altmetric_id': row['altmetric_id'], f'{source}_num': len(articles), f'{source}s': articles}\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_mention('tweet')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{origin_folder}/article_tweets_altmetric.json', \"w+\") as dump_f:\n",
    "    dump_f.write(json.dumps(articles_dict))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{origin_folder}/article_tweets_altmetric.json', \"r\") as f:\n",
    "    article_tweets = json.load(f)\n",
    "\n",
    "df_tweets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for doi in article_tweets:\n",
    "    for tweet_chain in article_tweets[doi]['tweets']:\n",
    "        for tweet in tweet_chain[1]:\n",
    "            if tweet['url'] in df_tweets['tweet_url']:\n",
    "                continue\n",
    "            df_tweet = pd.DataFrame(columns=['doi', 'altmetric_id', 'tweet_id', 'tweet_url', 'postedAt', 'postType', 'originalPost'])\n",
    "            df_tweet['doi'] = [doi]\n",
    "            df_tweet['altmetric_id'] = [article_tweets[doi]['altmetric_id']]\n",
    "            df_tweet['tweet_id'] = [str(tweet['url']).split('/')[-1]]\n",
    "            df_tweet['tweet_url'] = [tweet['url']]\n",
    "            df_tweet['postedAt'] = [tweet['postedAt']]\n",
    "            df_tweet['postType'] = [tweet['postType']]\n",
    "            df_tweet['originalPost'] = [str(tweet['originalPost'])]\n",
    "\n",
    "            df_tweets = df_tweets.append(df_tweet, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv(f'{origin_folder}/article_tweets_altmetric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(f'{origin_folder}/all_altmetric_detail.csv')\n",
    "df_tweets = pd.read_csv(f'{origin_folder}/article_tweets_altmetric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_count = df_tweets.groupby(by=['doi'])['altmetric_id'].count()\n",
    "df_all = df_all.merge(df_tweets_count, left_on=['DI'], right_on=['doi'], how='left', suffixes=['', '_tweets_count'])\n",
    "df_all['altmetric_id_tweets_count'] = df_all['altmetric_id_tweets_count'].fillna(0)\n",
    "df_all = df_all.rename({'altmetric_id_tweets_count': 'tweets_count'}, axis=1)\n",
    "df_tweets_count = df_tweets[df_tweets['originalPost'] == True].groupby(by=['doi'])['altmetric_id'].count()\n",
    "df_all = df_all.merge(df_tweets_count, left_on=['DI'], right_on=['doi'], how='left', suffixes=['', '_tweets_origin_count'])\n",
    "df_all['altmetric_id_tweets_origin_count'] = df_all['altmetric_id_tweets_origin_count'].fillna(0)\n",
    "df_all = df_all.rename({'altmetric_id_tweets_origin_count': 'tweets_origin_count'}, axis=1)\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = pd.read_csv(f'{origin_folder}/all.csv', usecols=['DI', 'SO', 'TC'])\n",
    "df_all = df_all.merge(all_info, on='DI', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(f'{origin_folder}/all_altmetric_detail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "articles_dict = {}\n",
    "get_mention('fbwall')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{origin_folder}/article_fbwalls_altmetric.json', \"w+\") as dump_f:\n",
    "    dump_f.write(json.dumps(articles_dict))"
   ]
  }
 ]
}