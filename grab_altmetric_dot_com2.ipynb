{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab Altmetric.com Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request Functions\n",
    "This part contains functions we need to fetch the web data and should also handle the exceptions while fetching here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grab_util import *\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_folder = 'SUSTC_Journals/articles_all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab altmetric.com Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_altmetric_id(doi):\n",
    "    detail_id = ''\n",
    "    score = 0\n",
    "    res = grab_from_url_json('https://api.altmetric.com/v1/doi/' + doi)\n",
    "    if res is not None:\n",
    "        detail_id = res['altmetric_id']\n",
    "        score = res['score']\n",
    "    return detail_id, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_df = pd.read_csv(f'{origin_folder}/all.csv', usecols=['SO', 'DI'])\n",
    "df = pd.DataFrame(columns=['DI', 'altmetric_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_altmetric_ids(start_index, end_index):\n",
    "    dfy =pd.DataFrame()\n",
    "\n",
    "    all_articles_df_slice = all_articles_df[start_index:end_index]\n",
    "    for index, doi in enumerate(all_articles_df_slice['DI']):\n",
    "        if doi in df['DI'].values:\n",
    "            continue\n",
    "        if index % 500 == 0:\n",
    "             print(index)\n",
    "        dfx = pd.DataFrame(columns=['DI', 'altmetric_id', 'score'])\n",
    "        dfx['DI'] = [doi]\n",
    "        res = get_altmetric_id(str(doi))\n",
    "        dfx['altmetric_id'] = [res[0]]\n",
    "        dfx['score'] = [res[1]]\n",
    "        dfy = dfy.append(dfx, ignore_index=True)\n",
    "    return dfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grabIdThread(threading.Thread):\n",
    "    def __init__(self, threadID, name, start_index, end_index):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "    def run(self):\n",
    "        global df\n",
    "        dfy = grab_altmetric_ids(self.start_index, self.end_index)\n",
    "        threadLock.acquire()\n",
    "        df = df.append(dfy)\n",
    "        threadLock.release()\n",
    "\n",
    "threadLock = threading.Lock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "step = 1000\n",
    "threads = []\n",
    "\n",
    "prior_end = 0\n",
    "for i in range(0, 10):\n",
    "    for j in range(0, (int)(len(all_articles_df) / (step * 10))):\n",
    "        start_index = prior_end\n",
    "        end_index = start_index + step\n",
    "        if end_index + step > len(all_articles_df):\n",
    "            end_index = len(all_articles_df)\n",
    "\n",
    "        prior_end = end_index\n",
    "        \n",
    "        if start_index >= end_index:\n",
    "            break\n",
    "\n",
    "        threadx = grabIdThread(j, f'thread_{start_index}_{end_index}', start_index, end_index)\n",
    "        print(threadx)\n",
    "        threadx.start()\n",
    "        threads.append(threadx)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "df = df.sort_index()\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv(f'{origin_folder}/all_altmetric_id.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab altmetric.com Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_detail_altmetric(doi, altmetric_id):\n",
    "    df = pd.DataFrame()\n",
    "    b_list = ['news outlets', 'blogs', 'policy', 'tweeters', 'weibo', 'facebook pages', 'wikipedia', 'redditors', 'f1000', 'video uploader', 'dimensions_citation', 'mendeley', 'citeulike']\n",
    "\n",
    "    df['DI'] = [doi]\n",
    "    df['altmetric_id'] = [altmetric_id]\n",
    "    if altmetric_id != '' and not math.isnan(altmetric_id):\n",
    "        altmetric_id = int(altmetric_id)\n",
    "        news_anchor = 'news</dt><dd><a href=\"/details/' + str(altmetric_id) + '/news\"><strong>'\n",
    "        blogs_anchor = 'blogs</dt><dd><a href=\"/details/' + str(altmetric_id) + '/blogs\"><strong>'\n",
    "        policy_anchor = 'policy</dt><dd><a href=\"/details/' + str(altmetric_id) + '/policy-documents\"><strong>'\n",
    "        twitter_anchor = 'twitter</dt><dd><a href=\"/details/' + str(altmetric_id) + '/twitter\"><strong>'\n",
    "        weibo_anchor = 'weibo</dt><dd><a href=\"/details/' + str(altmetric_id) + '/weibo\"><strong>'\n",
    "        facebook_anchor = 'facebook</dt><dd><a href=\"/details/' + str(altmetric_id) + '/facebook\"><strong>'\n",
    "        wikipedia_anchor = 'wikipedia</dt><dd><a href=\"/details/' + str(altmetric_id) + '/wikipedia\"><strong>'\n",
    "        redditors_anchor = 'reddit</dt><dd><a href=\"/details/' + str(altmetric_id) + '/reddit\"><strong>'\n",
    "        f1000_anchor = 'f1000</dt><dd><a href=\"/details/' + str(altmetric_id) + '/f1000\"><strong>'\n",
    "        video_anchor = 'video</dt><dd><a href=\"/details/' + str(altmetric_id) + '/video\"><strong>'\n",
    "        dimensions_citation_anchor = 'dimensions_citation</dt><dd><a href=\"/details/' + str(altmetric_id) + '/citations\"><strong>'\n",
    "        mendeley_anchor = 'mendeley</dt><dd><a href=\"/details/' + str(altmetric_id) + '#mendeley-demographics\"><strong>'\n",
    "        citeulike_anchor = 'citeulike</dt><dd><strong>'\n",
    "        c_list = [news_anchor, blogs_anchor, policy_anchor, twitter_anchor, weibo_anchor, facebook_anchor, wikipedia_anchor, redditors_anchor, f1000_anchor, video_anchor, dimensions_citation_anchor, mendeley_anchor, citeulike_anchor]\n",
    "\n",
    "        end_anchor = '</strong>'\n",
    "\n",
    "        res = grab_from_url_content('https://www.altmetric.com/details/' + str(altmetric_id))\n",
    "        if res is not None:\n",
    "\n",
    "            for i in range(0, len(c_list)):\n",
    "                start_index = res.find(c_list[i])\n",
    "                if start_index > 0:\n",
    "                    start_index += len(c_list[i])\n",
    "                    number = 0\n",
    "                    end_index = res.find(end_anchor, start_index, start_index + 100)\n",
    "                    number_temp = res[start_index: end_index]\n",
    "\n",
    "                    if number_temp is not '':\n",
    "                        number = number_temp\n",
    "                    df[b_list[i]] = int(number)\n",
    "                else:\n",
    "                    df[b_list[i]] = 0\n",
    "        else:\n",
    "            for i in range(0, len(b_list)):\n",
    "                df[b_list[i]] = 0\n",
    "    else:\n",
    "        for i in range(0, len(b_list)):\n",
    "            df[b_list[i]] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_altmetric_detail_slice(start_index, end_index):\n",
    "    dfy =pd.DataFrame()\n",
    "\n",
    "    all_articles_df_slice = all_articles_df[start_index:end_index]\n",
    "    for index, row in all_articles_df_slice.iterrows():\n",
    "        if row['DI'] in dfg['DI'].values:\n",
    "            continue\n",
    "        if index % 100 == 0:\n",
    "            print(index)\n",
    "        dfx = grab_detail_altmetric(row['DI'], row['altmetric_id'])\n",
    "        dfy = dfy.append(dfx, ignore_index=True)\n",
    "    return dfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grabDetailThread(threading.Thread):\n",
    "    def __init__(self, threadID, name, start_index, end_index):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "    def run(self):\n",
    "        global dfg\n",
    "        dfy = grab_altmetric_detail_slice(self.start_index, self.end_index)\n",
    "        threadLock.acquire()\n",
    "        dfg = dfg.append(dfy)\n",
    "        threadLock.release()\n",
    "\n",
    "threadLock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "step = 1000\n",
    "threads = []\n",
    "all_articles_df = pd.read_csv(f'{origin_folder}/all_altmetric_id.csv')\n",
    "\n",
    "prior_end = 0\n",
    "for i in range(0, 20 + 1):\n",
    "    print(i)\n",
    "    dfg = pd.DataFrame(columns=['DI'])\n",
    "    for j in range(0, (int)(len(all_articles_df) / (step * 20))):\n",
    "        start_index = prior_end\n",
    "        end_index = start_index + step\n",
    "        if end_index + step > len(all_articles_df):\n",
    "            end_index = len(all_articles_df)\n",
    "        prior_end = end_index\n",
    "\n",
    "        if(start_index >= end_index):\n",
    "            break\n",
    "        \n",
    "        threadx = grabDetailThread(j, f'thread_{start_index}_{end_index}', start_index, end_index)\n",
    "        print(threadx)\n",
    "        threadx.start()\n",
    "        threads.append(threadx)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    dfg = dfg.sort_index()\n",
    "    dfg = dfg.drop_duplicates()\n",
    "    dfg.to_csv(f'{origin_folder}/all_altmetric_detail_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab altmetric.com's Tweets Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "We will get the html content from the url which is not listed as we want it be, so we need parser to parse them into listed data, in json form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class AltmetricHTMLParser(HTMLParser):\n",
    "    tweets = []\n",
    "    retweets = []\n",
    "    articles = {'tweets': tweets, 'retweets': retweets}\n",
    "    in_article = False\n",
    "    is_reply = False\n",
    "    has_article = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'body':\n",
    "            self.has_article = False\n",
    "        if tag == 'article':\n",
    "            self.has_article = True\n",
    "            self.in_article = True\n",
    "\n",
    "        if self.in_article and (tag == 'a'):\n",
    "            for attr in attrs:\n",
    "                if (attr[0] == 'class') and (attr[1] == 'reply'):\n",
    "                    self.is_reply = True\n",
    "                if self.is_reply and (attr[0] == 'href'):\n",
    "                    self.tweets.append(attr[1].split('=')[1])\n",
    "                    break\n",
    "        return\n",
    "                    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'article':\n",
    "            self.in_article = False\n",
    "        self.is_reply = False\n",
    "        return\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        pass\n",
    "\n",
    "    def handle_comment(self, data):\n",
    "        pass\n",
    "\n",
    "    def handle_entityref(self, name):\n",
    "        pass\n",
    "\n",
    "    def handle_charref(self, name):\n",
    "        pass\n",
    "\n",
    "    def handle_decl(self, data):\n",
    "        pass\n",
    "\n",
    "parser = AltmetricHTMLParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention(source, dfx):\n",
    "    headers = {\n",
    "        'cookie': \"_ga=GA1.2.501259425.1572141776; weibo_license_acknowledgement=false; explorer_user=WWs2eDNOOUVjVHJTcHRjSm9Oc0ZuZEh0eDRkTmlPd0tCWFMzODNyNlIweFBvdjllM3RkcmF0RVdtRWdvRDN4Ny0tQlNpeTBsWU5NK0ZNTnFpK1FpVFUzdz09--8bac413ca9fd623b9918b311729130cfbb4e66e1; _altmetric-explorer_session=eExKU3g1UmJhWXpXbmVtS05YZmxVS3YyaVNDSGdPL2JyaWVYaE9mRTdPT3RlYlRLaTdrRmRWcnZpeFMrMFJLb0FIVDBvNHI3N09MWS9HZDhDRDdIbTZPZElvMWNPNzMvTm5JVkovWThiQVplZVdTRlZheDBWYWdvd2lIMXo2NHJUZkFJd2MxcWpZSmpCaXVXRW9CYlZnZ1l4L1lXMGhIa1k1d0o1bllvS09uWDBQb3JXMWUrazZ2YWVwaUwzei93cDdUZkhwanFtSy9lSkxLUmdRdEIrZz09LS1DckIxZFJZUVBWNmhsTEJXcWtkR2x3PT0%3D--884ad24217914b545556a68678c2d803c65ce9cf; intercom-session-9dnltu6y=alcwSlZxZHMzZlY3Wm8yaFpvWFg1OGxEYVZ4MXRRUDMrbkk1Q01Bdm5XNXJkQXNZcVAyeEZYVHFYS3RUbWJSbi0tclFPUDlZcUVpMFZKQVlxdlMwRllVQT09--46bb4ebcf64063d6a3712dd82a1d2bff57eed87f, _ga=GA1.2.501259425.1572141776; weibo_license_acknowledgement=false; explorer_user=WWs2eDNOOUVjVHJTcHRjSm9Oc0ZuZEh0eDRkTmlPd0tCWFMzODNyNlIweFBvdjllM3RkcmF0RVdtRWdvRDN4Ny0tQlNpeTBsWU5NK0ZNTnFpK1FpVFUzdz09--8bac413ca9fd623b9918b311729130cfbb4e66e1; _altmetric-explorer_session=eExKU3g1UmJhWXpXbmVtS05YZmxVS3YyaVNDSGdPL2JyaWVYaE9mRTdPT3RlYlRLaTdrRmRWcnZpeFMrMFJLb0FIVDBvNHI3N09MWS9HZDhDRDdIbTZPZElvMWNPNzMvTm5JVkovWThiQVplZVdTRlZheDBWYWdvd2lIMXo2NHJUZkFJd2MxcWpZSmpCaXVXRW9CYlZnZ1l4L1lXMGhIa1k1d0o1bllvS09uWDBQb3JXMWUrazZ2YWVwaUwzei93cDdUZkhwanFtSy9lSkxLUmdRdEIrZz09LS1DckIxZFJZUVBWNmhsTEJXcWtkR2x3PT0%3D--884ad24217914b545556a68678c2d803c65ce9cf; intercom-session-9dnltu6y=alcwSlZxZHMzZlY3Wm8yaFpvWFg1OGxEYVZ4MXRRUDMrbkk1Q01Bdm5XNXJkQXNZcVAyeEZYVHFYS3RUbWJSbi0tclFPUDlZcUVpMFZKQVlxdlMwRllVQT09--46bb4ebcf64063d6a3712dd82a1d2bff57eed87f; Cookie_1=value\",\n",
    "        'User-Agent': \"PostmanRuntime/7.19.0\",\n",
    "        'Accept': \"*/*\",\n",
    "        'Cache-Control': \"no-cache\",\n",
    "        'Postman-Token': \"30d13c93-aa9a-44e1-84e0-8876f98ae1ba,b45c8eb6-7f77-4c48-b961-921c89280079\",\n",
    "        'Host': \"www.altmetric.com\",\n",
    "        'Accept-Encoding': \"gzip, deflate\",\n",
    "        'Connection': \"keep-alive\",\n",
    "        'cache-control': \"no-cache\"\n",
    "    }\n",
    "\n",
    "    articles_dictx = {}\n",
    "    \n",
    "    for index, row in dfx.iterrows():\n",
    "        if index % 100 == 0:\n",
    "            print(str(index))\n",
    "        if row['DI'] in articles_dict:\n",
    "            continue\n",
    "        if row['DI'] == '' or str(row['DI']).lower() == 'nan' or row['altmetric_id'] == '' or str(row['altmetric_id']).lower() == 'nan':\n",
    "            articles_dict[row['DI']] = {'altmetric_id': '', f'{source}_num': 0, f'{source}s': []}\n",
    "            continue\n",
    "\n",
    "        articles = []\n",
    "        grab_url = f'https://www.altmetric.com/explorer/json_data/mentions?identifier={row[\"DI\"]}&mention_sources%5B%5D=type%3A{source}&scope=all&page='\n",
    "        for i in range(1, 100000):\n",
    "            articles_json = grab_from_url_json(grab_url + str(i), headers=headers)\n",
    "            if articles_json == None:\n",
    "                break\n",
    "\n",
    "            articles.extend(articles_json['data'])\n",
    "            # print('page: ', str(i))\n",
    "            if articles_json['lastPage']:\n",
    "                break\n",
    "        \n",
    "        articles_dictx[row['DI']] = {'altmetric_id': row['altmetric_id'], f'{source}_num': len(articles), f'{source}s': articles}\n",
    "    return articles_dictx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grabTweetsThread(threading.Thread):\n",
    "    def __init__(self, threadID, name, dfx):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.dfx = dfx\n",
    "    def run(self):\n",
    "        articles_dictx = get_mention('tweet', self.dfx)\n",
    "        global articles_dict\n",
    "        threadLock.acquire()\n",
    "        articles_dict.update(articles_dictx)\n",
    "        threadLock.release()\n",
    "\n",
    "threadLock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(f'{origin_folder}/all_altmetric_id.csv', usecols=['DI', 'altmetric_id'], dtype={'altmetric_id':str})\n",
    "split1 = 4\n",
    "split2 = 500\n",
    "step = (int)(len(df) / (split1*split2))\n",
    "\n",
    "threads = []\n",
    "\n",
    "for i in range(0, split1):\n",
    "    articles_dict = {}\n",
    "    for j in range(0, split2):\n",
    "        start_index = i * split2 * step + j * step\n",
    "        end_index = start_index + step\n",
    "        if end_index + step > len(df):\n",
    "            end_index = len(df)\n",
    "        \n",
    "        dfx = df[start_index:end_index]\n",
    "        threadx = grabTweetsThread(j, f'thread_{start_index}_{end_index}', dfx)\n",
    "        print(threadx)\n",
    "        threadx.start()\n",
    "        threads.append(threadx)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    with open(f'{origin_folder}/article_tweets_altmetric_{i}.json', \"w+\") as dump_f:\n",
    "        dump_f.write(json.dumps(articles_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del articles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_tweets(i):\n",
    "    print(i)\n",
    "    with open(f'{origin_folder}/article_tweets_altmetric_{i}.json', \"r\") as f:\n",
    "        article_tweets = json.load(f)\n",
    "\n",
    "    df_tweets = pd.DataFrame(columns=['doi', 'altmetric_id', 'tweet_id', 'tweet_url', 'postedAt', 'postType', 'originalPost'])\n",
    "    saved_count = 0\n",
    "    doi_count = 0\n",
    "\n",
    "    for doi in article_tweets:\n",
    "        doi_count += 1\n",
    "        if doi_count % 500 == 0:\n",
    "            print(doi_count)\n",
    "        for tweet_chain in article_tweets[doi]['tweets']:\n",
    "            for tweet in tweet_chain[1]:\n",
    "                if tweet['url'] in df_tweets['tweet_url']:\n",
    "                    continue\n",
    "                df_tweet = pd.DataFrame(columns=['doi', 'altmetric_id', 'tweet_id', 'tweet_url', 'postedAt', 'postType', 'originalPost'])\n",
    "                df_tweet['doi'] = [doi]\n",
    "                df_tweet['altmetric_id'] = [article_tweets[doi]['altmetric_id']]\n",
    "                df_tweet['tweet_id'] = [str(tweet['url']).split('/')[-1]]\n",
    "                df_tweet['tweet_url'] = [tweet['url']]\n",
    "                df_tweet['postedAt'] = [tweet['postedAt']]\n",
    "                df_tweet['postType'] = [tweet['postType']]\n",
    "                df_tweet['originalPost'] = [str(tweet['originalPost'])]\n",
    "\n",
    "                df_tweets = df_tweets.append(df_tweet, ignore_index=True)\n",
    "\n",
    "        if len(df_tweets) > 200000:\n",
    "            df_tweets.to_csv(f'{origin_folder}/article_tweets_altmetric_{i}_{saved_count}.csv', index=False)\n",
    "            df_tweets = pd.DataFrame(columns=['doi', 'altmetric_id', 'tweet_id', 'tweet_url', 'postedAt', 'postType', 'originalPost'])\n",
    "            saved_count += 1\n",
    "\n",
    "    df_tweets.to_csv(f'{origin_folder}/article_tweets_altmetric_{i}_{saved_count}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(0, 4):\n",
    "    order_tweets(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(f'{origin_folder}/all_altmetric_detail.csv')\n",
    "df_tweets = pd.read_csv(f'{origin_folder}/article_tweets_altmetric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_count = df_tweets.groupby(by=['doi'])['altmetric_id'].count()\n",
    "df_all = df_all.merge(df_tweets_count, left_on=['DI'], right_on=['doi'], how='left', suffixes=['', '_tweets_count'])\n",
    "df_all['altmetric_id_tweets_count'] = df_all['altmetric_id_tweets_count'].fillna(0)\n",
    "df_all = df_all.rename({'altmetric_id_tweets_count': 'tweets_count'}, axis=1)\n",
    "df_tweets_count = df_tweets[df_tweets['originalPost'] == True].groupby(by=['doi'])['altmetric_id'].count()\n",
    "df_all = df_all.merge(df_tweets_count, left_on=['DI'], right_on=['doi'], how='left', suffixes=['', '_tweets_origin_count'])\n",
    "df_all['altmetric_id_tweets_origin_count'] = df_all['altmetric_id_tweets_origin_count'].fillna(0)\n",
    "df_all = df_all.rename({'altmetric_id_tweets_origin_count': 'tweets_origin_count'}, axis=1)\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(f'{origin_folder}/all_altmetric_detail.csv')\n",
    "df_score = pd.read_csv(f'{origin_folder}/all_altmetric_id_score.csv', usecols=['DI', 'score'])\n",
    "df_all = df_all.merge(df_score, on='DI', how='left')\n",
    "df_all['has_alt_id'] = df_all['altmetric_id'].isnull()\n",
    "df_all['has_alt_id'] = df_all['has_alt_id'].apply(lambda x: 0 if x else 1)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = pd.read_csv(f'{origin_folder}/all.csv', usecols=['DI', 'SO'])\n",
    "df_all = df_all.merge(all_info, on='DI', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(f'{origin_folder}/all_altmetric_detail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "articles_dict = {}\n",
    "get_mention('fbwall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{origin_folder}/article_fbwalls_altmetric.json', \"w+\") as dump_f:\n",
    "    dump_f.write(json.dumps(articles_dict))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2.0,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3.0
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
